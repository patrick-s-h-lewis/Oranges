{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Generator\n",
    "Generates a corpus for training using the titles of CamHarvestCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import string\n",
    "import codecs\n",
    "import gensim \n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "punct_filter = [u'\"',u'#',u'$',u'%',u'&',u'\\\\',u\"'\",u'(',u')',u'*',u'+',u',',u'.',u'/',\n",
    "     u'-',u':',u';',u'<',u'=',u'>',u'?',u'@',u'[',u']',u'^',u'_',u'`',u'{',\n",
    "     u'|',u'}',u'â€“',u'\\u2013',u'\\u2010',u'\\u2606',u'\\u201D',u'\\u2248',u'\\u223C',u'\\u2212',u'\\u2014',u'\\u2032',u'\\u2018',u'\\u2019',u'\\u2022',u'\\u2020',u'\\u00B0',u'\\u29B9',u'\\uFF0D',u'\\u2261']\n",
    "stop = stopwords.words('english')\n",
    "with open('chemistry_stopwords.json') as f:\n",
    "    chem_stop = json.load(f)\n",
    "max_stop = stop+chem_stop\n",
    "#mongo_url = 'mongodb://localhost:6666/'\n",
    "mongo_url = 'mongodb://localhost:27017/'\n",
    "db = 'Cherry'\n",
    "coll_in = 'Cranberry'\n",
    "client = MongoClient(mongo_url)\n",
    "ch = client[db][coll_in]\n",
    "coops = client[db]['raspberry']\n",
    "#corpusfile = 'corpus2.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GensimCorpus(object):\n",
    "    def __init__(self,corpus_text_file,diction):\n",
    "        self.corpus_text_file = corpus_text_file\n",
    "        self.dictionary = diction\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in open(self.corpus_text_file):\n",
    "            yield self.dictionary.doc2bow(line.split())\n",
    "\n",
    "def dictionary_generator(corpus_file):\n",
    "    dictionary = corpora.Dictionary(line.split() for line in open(corpus_file))\n",
    "    return dictionary\n",
    "\n",
    "def create_models(corpus_file):\n",
    "    dictionary = dictionary_generator(corpus_file)\n",
    "    print('Created Dictionary')\n",
    "    corp = GensimCorpus(corpus_file,dictionary)\n",
    "    print('Created Corpus Object')\n",
    "    tfidf = models.TfidfModel(corp)\n",
    "    print('Created TFIDF Model')\n",
    "    tfidf_corp = tfidf[corp]\n",
    "    print('Created TFIDF Corpus')\n",
    "    return dictionary,corp,tfidf,tfidf_corp\n",
    "\n",
    "def load_models(dictionary_file,corpus_file,tfidf_file):\n",
    "    dictionary = corpora.Dictionary.load(dictionary_file)\n",
    "    corp = GensimCorpus(corpus_file,dictionary)\n",
    "    tfidf = models.TfidfModel.load(tfidf_file)\n",
    "    tfidf_corp = tfidf[corp]\n",
    "    return dictionary,corp,tfidf,tfidf_corp\n",
    "\n",
    "def tfidf_filtered_corpus_generator(corpus_filename,threshold):\n",
    "    corpus_filename = 'tfidf_filtered_'+str(threshold).strip('.')+'.txt'\n",
    "    ind=0\n",
    "    with codecs.open(corpus_filename,'a',encoding='utf8') as f:\n",
    "        for doc in tfidf_corp:\n",
    "            if ind%500000 == 0:\n",
    "                print(ind)\n",
    "            f.write(' '.join([dictionary[i] for i,j in doc if j>=threshold]))\n",
    "            f.write('\\n')\n",
    "            ind+=1\n",
    "\n",
    "def raw_corpus_generator(file_name):\n",
    "    ind = 0 \n",
    "    with codecs.open(file_name,'a',encoding='utf8') as f:\n",
    "        for rec in ch.find({'crossref_doi':True}):\n",
    "            lt = rec['title'].lower()\n",
    "            slt = lt.strip()\n",
    "            tslt = slt.translate(punct_filter)\n",
    "            export = tslt+u'\\n'\n",
    "            f.write(export)\n",
    "            ind+=1\n",
    "            if ind%100000==0:\n",
    "                print(ind)\n",
    "\n",
    "def remove_unicode_punct(subj, chars):\n",
    "    return re.sub(u'(?u)[' + re.escape(''.join(chars)) + ']', ' ', subj)\n",
    "                \n",
    "def sanitise(title):\n",
    "    lt = title.lower()\n",
    "    slt = lt.strip()\n",
    "    tslt = remove_unicode_punct(slt,punct_filter)\n",
    "    stop_filtered = [i for i in tslt.split() if i not in stop]\n",
    "    export = u' '.join(stop_filtered)\n",
    "    return export\n",
    "\n",
    "def create_stopword_filtered_corpus(file_name):\n",
    "    ind = 0 \n",
    "    with codecs.open(file_name,'a',encoding='utf8') as f:\n",
    "        for rec in ch.find({'crossref_doi':True}):\n",
    "            f.write(sanitise(rec['title'])+'u\\n')\n",
    "            ind+=1\n",
    "            if ind%10000==0:\n",
    "                print(ind)\n",
    "                \n",
    "def create_stopword_filtered_raspberry_corpus(file_name):\n",
    "    ind = 0 \n",
    "    with codecs.open(file_name,'a',encoding='utf8') as f:\n",
    "        for rec in coops.find({'abstract': {'$exists': True}, '$where': \"this.abstract.length>0\"}):\n",
    "            san_title = sanitise(rec['title'])\n",
    "            san_abs = sanitise(rec['abstract'])\n",
    "            f.write(san_title+' '+san_abs+'\\n')\n",
    "            ind+=1\n",
    "            if ind%10000==0:\n",
    "                print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_corpus_stats(in_file,diction,outfile_name):\n",
    "    unique_word_count=0\n",
    "    for k in diction.iterkeys():\n",
    "        if unique_word_count<k:\n",
    "            unique_word_count=k\n",
    "    print('Counted Unique Words')\n",
    "    word_freq = Counter()\n",
    "    word_count = 0\n",
    "    document_count = 0\n",
    "    document_lengths = Counter()\n",
    "    interim_corp = GensimCorpus(in_file,diction)\n",
    "    ind=0\n",
    "    for doc in interim_corp:\n",
    "        word_count+=len(doc)\n",
    "        document_count+=1\n",
    "        document_lengths.update([len(doc)])\n",
    "        upd = []\n",
    "        for w_id,w_freq in doc:\n",
    "            upd+=([w_id]*w_freq)\n",
    "        word_freq.update(upd)\n",
    "        ind+=1\n",
    "        if ind%10000==0:\n",
    "            sys.stdout.write('\\r[{0}] {1}'.format('#'*(ind/10000), ind))\n",
    "            sys.stdout.flush()\n",
    "    mean_doc_length = float(word_count)/float(document_count)\n",
    "    mode_doc_length = document_lengths.most_common(1)[0]\n",
    "    print('\\nGenerated Counting Stats')\n",
    "    ranked_word_freq = word_freq.most_common()\n",
    "    ziphian_table = []\n",
    "    for rank in range(unique_word_count):\n",
    "        w = dictionary[ranked_word_freq[rank][0]].encode('utf-8')\n",
    "        r = rank+1\n",
    "        log_r = math.log(r,10)\n",
    "        f = ranked_word_freq[rank][1]\n",
    "        log_f = math.log(f,10)\n",
    "        ziphian_table.append((w,r,log_r,f,log_f))\n",
    "    print('Generated Ziphian Data')\n",
    "    sample = []\n",
    "    log_ranks = list(zip(*ziphian_table)[2])\n",
    "    for s in np.arange(ziphian_table[0][2],ziphian_table[-1][2],ziphian_table[1][2]):   \n",
    "        sample.append(log_ranks.index(min(log_ranks,key=lambda x:abs(x-s))))\n",
    "    z_grad,z_c = np.polyfit([ziphian_table[s][2] for s in sample],[ziphian_table[s][4] for s in sample],1)\n",
    "    line_freq = map(lambda x: z_grad*x+z_c,log_ranks)\n",
    "    plt.close()\n",
    "    plt.plot(log_ranks,list(zip(*ziphian_table)[4]),'r')\n",
    "    plt.plot(log_ranks,line_freq,'b')\n",
    "    plt.savefig(outfile_name+'_ziphian_plot.png')\n",
    "    print('Saved Ziphian Plot')\n",
    "    plt.close()\n",
    "    plt.plot(document_lengths.keys(),document_lengths.values())\n",
    "    plt.savefig(outfile_name+'_document_word_lengths.png')\n",
    "    plt.close()\n",
    "    print('Saved Document Length Distribution Plot')\n",
    "    with open(outfile_name+'_ziphian_data.csv', 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['word','rank','log rank','freqency','log_frequncy'])\n",
    "        writer.writerows(ziphian_table)\n",
    "    print('Writen Ziphian Data To file')\n",
    "    with open(outfile_name+'_document_lengths.csv', 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['words per document','number of documents'])\n",
    "        writer.writerows(zip(document_lengths.keys(),document_lengths.values()))\n",
    "    print('Writen Document Length Distribution data to file')\n",
    "    with open(outfile_name+'stats.txt','wb') as f:\n",
    "        f.write('Word count : '+str(word_count)+'\\n')\n",
    "        f.write('Unique words : ' + str(unique_word_count)+'\\n')\n",
    "        f.write('Mean document word count : ' + str(mean_doc_length)+'\\n')\n",
    "        f.write('Mode document word count : '+ str(mode_doc_length[0])+'\\n')\n",
    "        f.write('Document count : ' + str(document_count)+'\\n')\n",
    "        f.write('Ziphian gradient : '+str(z_grad)+'\\n')\n",
    "        f.write('Ziphian intercept : '+str(z_c)+'\\n')\n",
    "        f.write('most_frequent 10 words : '+'\\n')\n",
    "        for w in ziphian_table[0:10]:\n",
    "            f.write('\"'+w[0]+'\" : '+str(w[3])+' occurances\\n')\n",
    "    print('Written stats report to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted Unique Words\n",
      "[############################################] 440000Generated Counting Stats\n",
      "Generated Ziphian Data\n",
      "Saved Ziphian Plot\n",
      "Saved Document Length Distribution Plot\n",
      "Writen Ziphian Data To file\n",
      "Writen Document Length Distribution data to file\n",
      "Written stats report to file\n"
     ]
    }
   ],
   "source": [
    "#create_stopword_filtered_raspberry_corpus('second_raspberry_corpus.txt')\n",
    "#dictionary, corpus, tfidf_model,tfidf_corpus = create_models('second_raspberry_corpus.txt')\n",
    "#dictionary.save('second_raspberry_dictionary')\n",
    "#tfidf_model.save('second_raspberry_tfidf_model')\n",
    "#tfidf_corpus.save('second_raspberry_tfidf_corpus')\n",
    "dictionary,corp,tfidf,tfidf_corp = load_models('second_raspberry_dictionary','second_raspberry_corpus.txt','second_raspberry_tfidf_model')\n",
    "get_corpus_stats('second_raspberry_corpus.txt',dictionary,'RASPBERRY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def minimal_sanitise(title):\n",
    "    lt = title.lower()\n",
    "    slt = lt.strip()\n",
    "    tslt = remove_unicode_punct(slt,punct_filter)\n",
    "    export = tslt.strip()\n",
    "    return export\n",
    "\n",
    "def maximal_sanitise(title):\n",
    "    lt = title.lower()\n",
    "    slt = lt.strip()\n",
    "    tslt = remove_unicode_punct(slt,punct_filter)\n",
    "    stop_filtered = [i for i in tslt.split() if i not in max_stop]\n",
    "    export = u' '.join(stop_filtered)\n",
    "    return export\n",
    "\n",
    "\n",
    "def create_custom_raspberry_corpus(file_name,san_fun):\n",
    "    ind = 0 \n",
    "    with codecs.open(file_name,'a',encoding='utf8') as f:\n",
    "        for rec in coops.find({'abstract': {'$exists': True}, '$where': \"this.abstract.length>0\"}):\n",
    "            san_title = san_fun(rec['title'])\n",
    "            san_abs = san_fun(rec['abstract'])\n",
    "            f.write(san_title+' '+san_abs+'\\n')\n",
    "            ind+=1\n",
    "            if ind%10000==0:\n",
    "                print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "Created Dictionary\n",
      "Created Corpus Object\n",
      "Created TFIDF Model\n",
      "Created TFIDF Corpus\n",
      "Counted Unique Words\n",
      "[############################################] 440000\n",
      "Generated Counting Stats\n",
      "Generated Ziphian Data\n",
      "Saved Ziphian Plot\n",
      "Saved Document Length Distribution Plot\n",
      "Writen Ziphian Data To file\n",
      "Writen Document Length Distribution data to file\n",
      "Written stats report to file\n",
      "Created Dictionary\n",
      "Created Corpus Object\n",
      "Created TFIDF Model\n",
      "Created TFIDF Corpus\n",
      "Counted Unique Words\n",
      "[############################################] 440000\n",
      "Generated Counting Stats\n",
      "Generated Ziphian Data\n",
      "Saved Ziphian Plot\n",
      "Saved Document Length Distribution Plot\n",
      "Writen Ziphian Data To file\n",
      "Writen Document Length Distribution data to file\n",
      "Written stats report to file\n"
     ]
    }
   ],
   "source": [
    "create_custom_raspberry_corpus('raw_raspberry_corpus.txt',minimal_sanitise)\n",
    "create_custom_raspberry_corpus('chem_stop_raspberry_corpus.txt',maximal_sanitise)\n",
    "\n",
    "dictionary, corpus, tfidf_model,tfidf_corpus = create_models('raw_raspberry_corpus.txt')\n",
    "dictionary.save('raw_raspberry_dictionary')\n",
    "tfidf_model.save('raw_raspberry_tfidf_model')\n",
    "tfidf_corpus.save('raw_raspberry_tfidf_corpus')\n",
    "#dictionary,corp,tfidf,tfidf_corp = load_models('raw_raspberry_dictionary','raw_raspberry_corpus.txt','raw_raspberry_tfidf_model')\n",
    "get_corpus_stats('raw_raspberry_corpus.txt',dictionary,'raw_raspberry')\n",
    "\n",
    "dictionary, corpus, tfidf_model,tfidf_corpus = create_models('chem_stop_raspberry_corpus.txt')\n",
    "dictionary.save('chem_stop_raspberry_dictionary')\n",
    "tfidf_model.save('chem_stop_raspberry_tfidf_model')\n",
    "tfidf_corpus.save('chem_stop_raspberry_tfidf_corpus')\n",
    "#dictionary,corp,tfidf,tfidf_corp = load_models('chem_stop_raspberry_dictionary','chem_stop_raspberry_corpus.txt','chem_stop_raspberry_tfidf_model')\n",
    "get_corpus_stats('chem_stop_raspberry_corpus.txt',dictionary,'chem_stop_raspberry')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "cat\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.stem.lancaster.LancasterStemmer()\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "wordnet = nltk.stem.WordNetLemmatizer()\n",
    "print(lancaster.stem('cats'))\n",
    "print(porter.stem('cats'))\n",
    "print(snowball.stem('cats'))\n",
    "print(wordnet.lemmatize('cats'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
