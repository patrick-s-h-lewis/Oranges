{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Generator\n",
    "Generates a corpus for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import string\n",
    "import codecs\n",
    "import gensim \n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk.stem\n",
    "punct_filter = [u'\"',u'#',u'$',u'%',u'&',u'\\\\',u\"'\",u'(',u')',u'*',u'+',u',',u'.',u'/',\n",
    "     u'-',u':',u';',u'<',u'=',u'>',u'?',u'@',u'[',u']',u'^',u'_',u'`',u'{',\n",
    "     u'|',u'}',u'â€“',u'\\u2013',u'\\u2010',u'\\u2606',u'\\u22C5',u'\\u201D',u'\\u2248',u'\\u21CC',u'\\u223C',u'\\u2212',u'\\u2014',u'\\u2032',u'\\u2018',u'\\u2019',u'\\u2022',u'\\u2020',u'\\u00B0',u'\\u201C',u'\\u29B9',u'\\uFF0D',u'\\u2261']\n",
    "stop = stopwords.words('english')\n",
    "with open('chemistry_stopwords.json') as f:\n",
    "    chem_stop = json.load(f)\n",
    "max_stop = stop+chem_stop\n",
    "#mongo_url = 'mongodb://localhost:6666/'\n",
    "mongo_url = 'mongodb://localhost:27017/'\n",
    "db = 'Cherry'\n",
    "coll_in = 'Cranberry'\n",
    "client = MongoClient(mongo_url)\n",
    "ch = client[db][coll_in]\n",
    "coops = client[db]['raspberry']\n",
    "#corpusfile = 'corpus2.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GensimCorpus(object):\n",
    "    def __init__(self,corpus_text_file,diction):\n",
    "        self.corpus_text_file = corpus_text_file\n",
    "        self.dictionary = diction\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in open(self.corpus_text_file):\n",
    "            yield self.dictionary.doc2bow(line.split())\n",
    "\n",
    "def dictionary_generator(corpus_file):\n",
    "    dictionary = corpora.Dictionary(line.split() for line in open(corpus_file))\n",
    "    return dictionary\n",
    "\n",
    "def create_models(corpus_file):\n",
    "    dictionary = dictionary_generator(corpus_file)\n",
    "    print('Created Dictionary')\n",
    "    corp = GensimCorpus(corpus_file,dictionary)\n",
    "    print('Created Corpus Object')\n",
    "    tfidf = models.TfidfModel(corp)\n",
    "    print('Created TFIDF Model')\n",
    "    tfidf_corp = tfidf[corp]\n",
    "    print('Created TFIDF Corpus')\n",
    "    return dictionary,corp,tfidf,tfidf_corp\n",
    "\n",
    "def load_models(dictionary_file,corpus_file,tfidf_file):\n",
    "    dictionary = corpora.Dictionary.load(dictionary_file)\n",
    "    corp = GensimCorpus(corpus_file,dictionary)\n",
    "    tfidf = models.TfidfModel.load(tfidf_file)\n",
    "    tfidf_corp = tfidf[corp]\n",
    "    return dictionary,corp,tfidf,tfidf_corp\n",
    "\n",
    "def tfidf_filtered_corpus_generator(corpus_filename,threshold):\n",
    "    corpus_filename = 'tfidf_filtered_'+str(threshold).strip('.')+'.txt'\n",
    "    ind=0\n",
    "    with codecs.open(corpus_filename,'a',encoding='utf8') as f:\n",
    "        for doc in tfidf_corp:\n",
    "            if ind%500000 == 0:\n",
    "                print(ind)\n",
    "            f.write(' '.join([dictionary[i] for i,j in doc if j>=threshold]))\n",
    "            f.write('\\n')\n",
    "            ind+=1\n",
    "\n",
    "def remove_unicode_punct(subj, chars):\n",
    "    return re.sub(u'(?u)[' + re.escape(''.join(chars)) + ']', ' ', subj)\n",
    "                \n",
    "def create_cranberry_corpus(file_name,sanitizer):\n",
    "    ind = 0 \n",
    "    with codecs.open(file_name,'a',encoding='utf8') as f:\n",
    "        for rec in ch.find({'crossref_doi':True}):\n",
    "            ex = sanitizer(rec['title'])+u'\\n'\n",
    "            f.write(export)\n",
    "            ind+=1\n",
    "            if ind%100000==0:\n",
    "                print(ind)\n",
    "    \n",
    "def create_raspberry_corpus(file_name,sanitizer):\n",
    "    ind = 0 \n",
    "    with codecs.open(file_name,'a',encoding='utf8') as f:\n",
    "        for rec in coops.find({'abstract': {'$exists': True}, '$where': \"this.abstract.length>0\"}):\n",
    "            san_title = sanitizer(rec['title'])\n",
    "            san_abs = sanitizer(rec['abstract'])\n",
    "            f.write(san_title+' '+san_abs+'\\n')\n",
    "            ind+=1\n",
    "            if ind%10000==0:\n",
    "                print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Filterers\n",
    "\n",
    "def stop_word_sanitise(title,stops):\n",
    "    #lower case, strip whitespace and carriages, remove stopwords, remove punctuation\n",
    "    lt = title.lower()\n",
    "    slt = lt.strip()\n",
    "    tslt = remove_unicode_punct(slt,punct_filter)\n",
    "    stop_filtered = [i for i in tslt.split() if i not in stops]\n",
    "    export = u' '.join(stop_filtered)\n",
    "    return export\n",
    "\n",
    "def minimal_sanitise(title):\n",
    "    #lower case, strip whitespace and carriages, remove punctuation\n",
    "    lt = title.lower()\n",
    "    slt = lt.strip()\n",
    "    tslt = remove_unicode_punct(slt,punct_filter)\n",
    "    export = tslt.strip()\n",
    "    return export\n",
    "\n",
    "def stemming_sanitise(title,stops,stemmer):\n",
    "    #lower case, strip whitespace and carriages, remove punctuation, remove stopwords, stem\n",
    "    lt = title.lower()\n",
    "    slt = lt.strip()\n",
    "    tslt = remove_unicode_punct(slt,punct_filter)\n",
    "    stop_filtered = [i for i in tslt.split() if i not in stops]\n",
    "    stem_filtered = [stemmer.stem(i) for i in stop_filtered]\n",
    "    export = u' '.join(stem_filtered)\n",
    "    return export\n",
    "    \n",
    "def lemmatizing_sanitise(title,stops,lemmatizer):\n",
    "    #lower case, strip whitespace and carriages, remove punctuation, remove stopwords, lemmatize\n",
    "    lt = title.lower()\n",
    "    slt = lt.strip()\n",
    "    tslt = remove_unicode_punct(slt,punct_filter)\n",
    "    stop_filtered = [i for i in tslt.split() if i not in stops]\n",
    "    lemma_filtered = [lemmatizer.lemmatize(words) for words in stop_filtered]\n",
    "    export = u' '.join(lemma_filtered)\n",
    "    return export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_corpus_stats(in_file,diction,outfile_name):\n",
    "    unique_word_count=0\n",
    "    for k in diction.iterkeys():\n",
    "        if unique_word_count<k:\n",
    "            unique_word_count=k\n",
    "    print('Counted Unique Words')\n",
    "    word_freq = Counter()\n",
    "    word_count = 0\n",
    "    document_count = 0\n",
    "    document_lengths = Counter()\n",
    "    interim_corp = GensimCorpus(in_file,diction)\n",
    "    ind=0\n",
    "    for doc in interim_corp:\n",
    "        word_count+=len(doc)\n",
    "        document_count+=1\n",
    "        document_lengths.update([sum(zip(*doc)[1])])\n",
    "        upd = []\n",
    "        for w_id,w_freq in doc:\n",
    "            upd+=([w_id]*w_freq)\n",
    "        word_freq.update(upd)\n",
    "        ind+=1\n",
    "        if ind%10000==0:\n",
    "            sys.stdout.write('\\r[{0}] {1}'.format('#'*(ind/10000), ind))\n",
    "            sys.stdout.flush()\n",
    "    mean_doc_length = float(word_count)/float(document_count)\n",
    "    mode_doc_length = document_lengths.most_common(1)[0]\n",
    "    print('\\nGenerated Counting Stats')\n",
    "    ranked_word_freq = word_freq.most_common()\n",
    "    ziphian_table = []\n",
    "    for rank in range(unique_word_count):\n",
    "        w = dictionary[ranked_word_freq[rank][0]].encode('utf-8')\n",
    "        r = rank+1\n",
    "        log_r = math.log(r,10)\n",
    "        f = ranked_word_freq[rank][1]\n",
    "        log_f = math.log(f,10)\n",
    "        ziphian_table.append((w,r,log_r,f,log_f))\n",
    "    print('Generated Ziphian Data')\n",
    "    sample = []\n",
    "    log_ranks = list(zip(*ziphian_table)[2])\n",
    "    for s in np.arange(ziphian_table[0][2],ziphian_table[-1][2],ziphian_table[1][2]):   \n",
    "        sample.append(log_ranks.index(min(log_ranks,key=lambda x:abs(x-s))))\n",
    "    z_grad,z_c = np.polyfit([ziphian_table[s][2] for s in sample],[ziphian_table[s][4] for s in sample],1)\n",
    "    line_freq = map(lambda x: z_grad*x+z_c,log_ranks)\n",
    "    plt.close()\n",
    "    plt.plot(log_ranks,list(zip(*ziphian_table)[4]),'r')\n",
    "    plt.plot(log_ranks,line_freq,'b')\n",
    "    plt.savefig(outfile_name+'_ziphian_plot.png')\n",
    "    print('Saved Ziphian Plot')\n",
    "    plt.close()\n",
    "    plt.plot(document_lengths.keys(),document_lengths.values(),'o')\n",
    "    plt.savefig(outfile_name+'_document_word_lengths.png')\n",
    "    plt.close()\n",
    "    print('Saved Document Length Distribution Plot')\n",
    "    with open(outfile_name+'_ziphian_data.csv', 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['word','rank','log rank','freqency','log_frequncy'])\n",
    "        writer.writerows(ziphian_table)\n",
    "    print('Writen Ziphian Data To file')\n",
    "    with open(outfile_name+'_document_lengths.csv', 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['words per document','number of documents'])\n",
    "        writer.writerows(zip(document_lengths.keys(),document_lengths.values()))\n",
    "    print('Writen Document Length Distribution data to file')\n",
    "    with open(outfile_name+'stats.txt','wb') as f:\n",
    "        f.write('Word count : '+str(word_count)+'\\n')\n",
    "        f.write('Unique words : ' + str(unique_word_count)+'\\n')\n",
    "        f.write('Mean document word count : ' + str(mean_doc_length)+'\\n')\n",
    "        f.write('Mode document word count : '+ str(mode_doc_length[0])+'\\n')\n",
    "        f.write('Document count : ' + str(document_count)+'\\n')\n",
    "        f.write('Ziphian gradient : '+str(z_grad)+'\\n')\n",
    "        f.write('Ziphian intercept : '+str(z_c)+'\\n')\n",
    "        f.write('most_frequent 10 words : '+'\\n')\n",
    "        for w in ziphian_table[0:10]:\n",
    "            f.write('\"'+w[0]+'\" : '+str(w[3])+' occurances\\n')\n",
    "    print('Written stats report to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "Created Dictionary\n",
      "Created Corpus Object\n",
      "Created TFIDF Model\n",
      "Created TFIDF Corpus\n",
      "Counted Unique Words\n",
      "[############################################] 440000\n",
      "Generated Counting Stats\n",
      "Generated Ziphian Data\n",
      "Saved Ziphian Plot\n",
      "Saved Document Length Distribution Plot\n",
      "Writen Ziphian Data To file\n",
      "Writen Document Length Distribution data to file\n",
      "Written stats report to file\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "create_raspberry_corpus('stemmed_raspberry_corpus.txt',lambda x: stemming_sanitise(x,max_stop,stemmer))\n",
    "dictionary, corpus, tfidf_model,tfidf_corpus = create_models('stemmed_raspberry_corpus.txt')\n",
    "dictionary.save('stemmed_raspberry_dictionary')\n",
    "tfidf_model.save('stemmed_raspberry_tfidf_model')\n",
    "tfidf_corpus.save('stemmed_raspberry_tfidf_corpus')\n",
    "#dictionary,corp,tfidf,tfidf_corp = load_models('raw_raspberry_dictionary','raw_raspberry_corpus.txt','raw_raspberry_tfidf_model')\n",
    "get_corpus_stats('stemmed_raspberry_corpus.txt',dictionary,'stemmed_raspberry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_stemmers(dictionary,outfile_name):\n",
    "    it = dictionary.iteritems()\n",
    "    lancaster_reps=0\n",
    "    porter_reps=0\n",
    "    snowball_reps=0\n",
    "    wordnet_reps=0\n",
    "    lancaster_porter_agreements = 0\n",
    "    lancaster_snowball_agreements = 0\n",
    "    lancaster_wordnet_agreements = 0\n",
    "    porter_snowball_agreements = 0\n",
    "    porter_wordnet_agreements = 0\n",
    "    snowball_wordnet_agreements=0\n",
    "    ind=0\n",
    "    with open(outfile_name, 'wb') as f:\n",
    "        writer = csv.writer(f, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['word','lancaster','porter','snowball','wordnet'])\n",
    "        for w_id,w in it:\n",
    "            wl = lancaster.stem(w)\n",
    "            wp = porter.stem(w)\n",
    "            ws = snowball.stem(w)\n",
    "            ww = wordnet.lemmatize(w)\n",
    "            wl_fire = w!=wl\n",
    "            wp_fire = w!=wp\n",
    "            ws_fire = w!=ws\n",
    "            ww_fire = w!=ww\n",
    "            lancaster_reps+=int(wl_fire)\n",
    "            porter_reps+=int(wp_fire)\n",
    "            snowball_reps+=int(ws_fire)\n",
    "            wordnet_reps+=int(ww_fire)\n",
    "            if (wl_fire|wp_fire|ws_fire|ww_fire):\n",
    "                row = [w.encode('utf-8'),'','','','']\n",
    "                if wl_fire: row[1]=wl.encode('utf-8')\n",
    "                if wp_fire: row[2]=wp.encode('utf-8')\n",
    "                if ws_fire: row[3]=ws.encode('utf-8')\n",
    "                if ww_fire: row[4]=ww.encode('utf-8')\n",
    "                writer.writerow(row)\n",
    "                lancaster_porter_agreements +=int(wl==wp)\n",
    "                lancaster_snowball_agreements +=int(wl==ws)\n",
    "                lancaster_wordnet_agreements +=int(wl==ww)\n",
    "                porter_snowball_agreements +=int(wp==ws)\n",
    "                porter_wordnet_agreements +=int(wp==ww)\n",
    "                snowball_wordnet_agreements +=int(ws==ww)\n",
    "            if ind%10000==0:\n",
    "                sys.stdout.write('\\r[{0}] {1}'.format('#'*(ind/10000), ind))\n",
    "                sys.stdout.flush()\n",
    "            ind+=1\n",
    "    print('finished')\n",
    "    print('lancaster replacements: '+str(lancaster_reps))\n",
    "    print('porter replacements: '+str(porter_reps))\n",
    "    print('snowball replacements: '+str(snowball_reps))\n",
    "    print('wordnet replacements: '+str(wordnet_reps))\n",
    "    print('lancaster_porter_agreements: ' + str(lancaster_porter_agreements))\n",
    "    print('lancaster_snowball_agreements: ' +str(lancaster_snowball_agreements))\n",
    "    print('lancaster_wordnet_agreements: '+str(lancaster_wordnet_agreements))\n",
    "    print('porter_snowball_agreements: '+str(porter_snowball_agreements))\n",
    "    print('porter_wordnet_agreements: '+str(porter_wordnet_agreements))\n",
    "    print('snowball_wordnet_agreements: '+str(snowball_wordnet_agreements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=1\n",
    "def y(x):\n",
    "    #return 95*(0.5*math.pi**(2.5)*110**1.5)*(x/400.)**2*math.exp(-0.5*110*(x/400.)**2)\n",
    "    return 5.991524457330881*x**2*math.exp(-0.00034375*x**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "#plt.plot(map(lambda i: i/400.,doc_x),[95*y(i,110) for i in map(lambda i: i/400.,doc_x)],)\n",
    "#plt.plot(map(lambda i: i/400.,doc_x),doc_y)\n",
    "plt.plot(doc_x,[y(i) for i in doc_x])\n",
    "plt.plot(doc_x,doc_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(x,y):\n",
    "    return (x-y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_y =[]\n",
    "doc_x=[]\n",
    "with open('stemmed_raspberry_document_lengths.csv','r') as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='|')\n",
    "    for row in reader:\n",
    "        try:\n",
    "            doc_x.append(int(row[0]))\n",
    "            doc_y.append(int(row[1]))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_y =doc_y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00034375\n"
     ]
    }
   ],
   "source": [
    "print round(-0.5*110./(400.**2),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Dictionary\n",
      "Created Corpus Object\n",
      "Created TFIDF Model\n",
      "Created TFIDF Corpus\n",
      "Counted Unique Words\n",
      "[##########] 100000\n",
      "Generated Counting Stats\n",
      "Generated Ziphian Data\n",
      "Saved Ziphian Plot\n",
      "Saved Document Length Distribution Plot\n",
      "Writen Ziphian Data To file\n",
      "Writen Document Length Distribution data to file\n",
      "Written stats report to file\n"
     ]
    }
   ],
   "source": [
    "#create_raspberry_corpus('corpnatural.txt',minimal_sanitise)\n",
    "dictionary, corpus, tfidf_model,tfidf_corpus = create_models('corpnatural.txt')\n",
    "dictionary.save('corpnatural_dictionary')\n",
    "tfidf_model.save('corpnatural_tfidf_model')\n",
    "tfidf_corpus.save('corpnatural_tfidf_corpus')\n",
    "#dictionary,corp,tfidf,tfidf_corp = load_models('raw_raspberry_dictionary','raw_raspberry_corpus.txt','raw_raspberry_tfidf_model')\n",
    "get_corpus_stats('corpnatural.txt',dictionary,'corpnatural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interim_corp = GensimCorpus('corpnatural.txt',dictionary)\n",
    "ind=0\n",
    "document_count=0\n",
    "document_lengths = []\n",
    "for doc in interim_corp:\n",
    "    document_count+=1\n",
    "    document_lengths.append(sum(zip(*doc)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(*[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)])[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((1,1,1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
